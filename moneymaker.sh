python -c "import os,subprocess,torch,re;subprocess.run(['pip','install','--upgrade','pip','torch','datasets','transformers','peft','trl'],check=True);from datasets import load_dataset;from transformers import AutoTokenizer,AutoModelForCausalLM;from peft import LoraConfig;from trl import GRPOConfig,GRPOTrainer;ds=load_dataset('gsm8k','main')['train'].map(lambda x:{'prompt':x['question'],'response':x['answer']},remove_columns=['question','answer']);tokenizer=AutoTokenizer.from_pretrained('Qwen/Qwen2.5-1.5B-Instruct');model=AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-1.5B-Instruct',torch_dtype=torch.bfloat16,attn_implementation='eager',device_map='cpu').to('mps');os.makedirs('outputs/Qwen-1.5B-GRPO',exist_ok=True);config=GRPOConfig(output_dir='outputs/Qwen-1.5B-GRPO',run_name='Qwen-1.5B-GRPO-gsm8k',learning_rate=5e-6,adam_beta1=0.9,adam_beta2=0.99,weight_decay=0.1,warmup_ratio=0.1,lr_scheduler_type='cosine',logging_steps=1,bf16=True,per_device_train_batch_size=1,gradient_accumulation_steps=4,num_generations=16,max_prompt_length=256,max_completion_length=786,num_train_epochs=1,save_steps=100,max_grad_norm=0.1,report_to='wandb',log_on_each_node=False);peft_config=LoraConfig(r=16,lora_alpha=64,target_modules=['q_proj','k_proj','v_proj','o_proj','up_proj','down_proj','gate_proj'],task_type='CAUSAL_LM',lora_dropout=0.05);count_xml=lambda text:text.count('<');extract_xml_answer=lambda text:(m:=re.search(r'<answer>\\s*(.*?)\\s*</answer>',text,re.DOTALL))and m.group(1)or'';reward_funcs=[lambda prompts,completions,**kw:[count_xml(p) for p in prompts],lambda prompts,completions,**kw:[1 if re.match(r'^<reasoning>\\\\n.*?\\\\n</reasoning>\\\\n<answer>\\\\n.*?\\\\n</answer>\\\\n$',p)else 0 for p in prompts],lambda prompts,completions,**kw:[1 if re.match(r'<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>',p)else 0 for p in prompts],lambda prompts,completions,**kw:[0.5 if r.isdigit() else 0 for r in completions],lambda prompts,completions,**kw:[2.0 if y==z else 0.0 for y,z in zip([extract_xml_answer(x) for x in prompts],completions)]];trainer=GRPOTrainer(model=model,processing_class=tokenizer,train_dataset=ds,reward_funcs=reward_funcs);trainer.train();"
